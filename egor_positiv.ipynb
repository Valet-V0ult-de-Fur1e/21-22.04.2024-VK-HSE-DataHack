{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f725184-6738-4054-9ca6-1857ca8fe56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl -o ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcccae4-1315-4630-866a-8a962a52b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords based on frequency: ['Способ', 'спасти', 'посевы', 'ледяного', 'дождя', 'назвал', 'беседе', 'словам', 'главный', 'вред', 'причинить', 'ледяной', 'дождь', 'посеянным', 'осенью', 'чесноку', 'моркови', 'свекле', 'посадкам', 'помешать', 'воздухообмену', 'дождь', 'прошел', 'дачник', 'убрал', 'снег', 'посадками', 'участке', 'снега', 'побыстрее', 'ликвидировать', 'ледяную', 'корку', 'образовавшуюся', 'поверхности', 'уточнил', 'Делать', 'грубым', 'механическим', 'путем', 'разбив', 'граблями', 'лучше', 'убираете', 'листву', 'участке', 'Избавившись', 'корки', 'убрав', 'образовавшуюся', 'мешанину', 'подальше', 'восстановите', 'снежный', 'покров', 'хорошенько', 'встряхивая', 'снег', 'сыпучим', 'слежавшимся', 'порекомендовал', 'Проливать', 'огород', 'горячей', 'водой', 'продолжил', 'Синьковский', 'ледяной', 'дождь', 'прошел', 'грядками', 'находящимися', 'нормальной', 'толщины', 'снежным', 'покровом', 'площади', 'грядок', 'сломать', 'образовавшийся', 'наст', 'посоветовал', 'наст', 'объяснил', 'агроном', 'разбить', 'фрагменты', 'убрать', 'поверхности', 'черенком', 'лопаты', 'пробить', 'отверстия', 'площади', 'грядок', 'быстро', 'заполнятся', 'снегом', 'должную', 'циркуляцию', 'воздуха', 'образом', 'равно', 'обеспечите', 'заверил', 'специалист', 'Синьковский', 'назвал', 'правила', 'выращивания', 'баклажанов', 'перцев', 'Агроном', 'рекомендовал', 'использовать', 'готовые', 'жирные', 'грунты', 'высоким', 'содержанием', 'органики']\n",
      "Keywords based on semantics: ['Способ', 'спасти', 'посевы', 'ледяного', 'дождя', 'назвал', 'беседе', 'aif.ru', '.', 'словам', ',', 'главный', 'вред', ',', 'причинить', 'ледяной', 'дождь', 'посеянным', 'осенью', 'чесноку', ',', 'моркови', ',', 'свекле', 'посадкам', '—', 'помешать', 'воздухообмену', '.', 'дождь', 'прошел', ',', 'дачник', 'убрал', 'снег', 'посадками', 'участке', 'снега', ',', 'побыстрее', 'ликвидировать', 'ледяную', 'корку', ',', 'образовавшуюся', 'поверхности', ',', 'уточнил', '.', '«', 'Делать', 'грубым', 'механическим', 'путем', ',', 'разбив', 'граблями', '—', 'лучше', ',', 'убираете', 'листву', 'участке', '.', 'Избавившись', 'корки', 'убрав', 'образовавшуюся', 'мешанину', 'подальше', ',', 'восстановите', 'снежный', 'покров', ',', 'хорошенько', 'встряхивая', 'снег', ',', 'сыпучим', ',', 'слежавшимся', '»', ',', '—', 'порекомендовал', '.', 'Проливать', 'огород', 'горячей', 'водой', ',', 'продолжил', 'Синьковский', ',', '.', 'ледяной', 'дождь', 'прошел', 'грядками', ',', 'находящимися', 'нормальной', 'толщины', 'снежным', 'покровом', ',', 'площади', 'грядок', 'сломать', 'образовавшийся', 'наст', ',', 'посоветовал', '.', 'наст', ',', 'объяснил', 'агроном', ',', 'разбить', 'фрагменты', 'убрать', 'поверхности', ',', 'черенком', 'лопаты', 'пробить', 'отверстия', 'площади', 'грядок', '.', '«', ',', 'быстро', 'заполнятся', 'снегом', '—', 'должную', 'циркуляцию', 'воздуха', 'образом', 'равно', 'обеспечите', '»', ',', '—', 'заверил', 'специалист', '.', 'Синьковский', 'назвал', 'правила', 'выращивания', 'баклажанов', 'перцев', '.', 'Агроном', 'рекомендовал', 'использовать', 'готовые', 'жирные', 'грунты', 'высоким', 'содержанием', 'органики', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "text = \"Способ спасти посевы после ледяного дождя он назвал в беседе с aif.ru. По его словам, главный вред, который может причинить ледяной дождь посеянным осенью чесноку, моркови, свекле и другим посадкам — помешать воздухообмену. Если дождь прошел после того, как дачник убрал снег над посадками или если на участке мало снега, то надо побыстрее ликвидировать ледяную корку, образовавшуюся на поверхности, уточнил он. «Делать это надо грубым механическим путем, разбив ее граблями — лучше теми, которыми вы убираете листву на участке. Избавившись от корки и убрав образовавшуюся мешанину подальше, восстановите снежный покров, хорошенько встряхивая снег, чтобы он был сыпучим, а не слежавшимся», — порекомендовал он. Проливать огород горячей водой, продолжил Синьковский, нельзя. Если же ледяной дождь прошел над грядками, находящимися под нормальной толщины снежным покровом, нужно по всей площади грядок сломать образовавшийся наст, посоветовал он. Этот наст, объяснил агроном, нужно разбить на фрагменты и убрать с поверхности, а потом черенком от лопаты пробить отверстия по всей площади грядок. «Ничего, что они быстро заполнятся снегом — должную циркуляцию воздуха вы таким образом все равно обеспечите», — заверил специалист. Ранее Синьковский назвал правила выращивания баклажанов и перцев. Агроном рекомендовал использовать готовые жирные грунты с высоким содержанием органики.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Извлекаем ключевые слова на основе частоты\n",
    "keywords_freq = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "# Извлекаем ключевые слова на основе веса встроенных векторов\n",
    "keywords_semantic = [token.text for token in doc if not token.is_stop and token.vector_norm > 0]\n",
    "\n",
    "print(\"Keywords based on frequency:\", keywords_freq)\n",
    "print(\"Keywords based on semantics:\", keywords_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbb100f-abb8-423b-93b7-c638e1ad1fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/tljh/user/lib/python3.9/site-packages (from requests) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests) (3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/tljh/user/lib/python3.9/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/tljh/user/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88812169-5e52-4623-b20d-8ac40527bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end Политика\n",
      "end Экономика\n",
      "end Общество\n",
      "end Кино\n",
      "end Законы\n",
      "end Телевидение\n",
      "end Люди\n",
      "end Бренды\n",
      "end Наука\n",
      "end Гаджеты\n",
      "end Соцсети\n",
      "end Технологии\n",
      "end Опросы\n",
      "end Транспорт\n",
      "end Погода\n",
      "end Рецепты\n",
      "end Мода\n",
      "end Красота\n",
      "1659.3625235557556\n"
     ]
    }
   ],
   "source": [
    "import requests as req \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class RiaParser:\n",
    "    '''Культура'''\n",
    "    routes = [\n",
    "        {\n",
    "            'category': 'Политика',\n",
    "            'keyword': 'politics'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Экономика',\n",
    "            'keyword': 'economy'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Общество',\n",
    "            'keyword': 'society'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Кино',\n",
    "            'keyword': 'organization_Gruppa_Kino'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Законы',\n",
    "            'keyword': 'tag_zakon'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Телевидение',\n",
    "            'keyword': 'category_televidenie'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Люди',\n",
    "            'keyword': 'keyword_ljudi'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Бренды',\n",
    "            'keyword': 'keyword_brendy'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Наука',\n",
    "            'keyword': 'tag_gadzhety'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Гаджеты',\n",
    "            'keyword': 'tag_gadzhety'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Соцсети',\n",
    "            'keyword': 'tag_Socseti'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Технологии',\n",
    "            'keyword': 'technology'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Опросы',\n",
    "            'keyword': 'polls'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Транспорт',\n",
    "            'keyword': 'tag_thematic_category_Transport'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Погода',\n",
    "            'keyword': 'category_pogoda'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Рецепты',\n",
    "            'keyword': 'category_retsepty'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Мода',\n",
    "            'keyword': 'tag_moda_2'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Красота',\n",
    "            'keyword': 'product_krasota'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame({'category': [], 'text': []})\n",
    "    f = open('out_parsed_data.csv', 'a', encoding='utf-8')\n",
    "    f.write('category')\n",
    "    f.write(';')\n",
    "    f.write('data')\n",
    "    f.write('\\n')\n",
    "    deq = deque()\n",
    "\n",
    "    @classmethod\n",
    "    def get_ref_refs(cls, n):\n",
    "        ret = []\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    @classmethod\n",
    "    def get_refs_page(cls, start_url, n):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_refs_news(cls, soup):\n",
    "        ret = []\n",
    "        # soup = BeautifulSoup(html, 'html.parser')\n",
    "        elements = soup.find_all('a', attrs={'class': 'list-item__title color-font-hover-only'})\n",
    "        for element in elements:\n",
    "            ret.append(element['href'])\n",
    "        return ret\n",
    "    \n",
    "    @classmethod\n",
    "    def get_text(cls, url):\n",
    "        if 1:\n",
    "            res = req.get(url)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            topics = soup.find_all('div', attrs={'class': 'article__block'})\n",
    "            topics_text = []\n",
    "            for topic in topics:\n",
    "                topics_text.append(topic.text)\n",
    "            return ' '.join(topics_text)\n",
    "        return ''\n",
    "\n",
    "    @classmethod\n",
    "    def save_to_df(cls, category, data):\n",
    "        cls.df.loc[len(cls.df)] = [category, data]\n",
    "\n",
    "    @classmethod\n",
    "    def save_to_file(cls, category, data):\n",
    "        cls.f.write(category)\n",
    "        cls.f.write(';')\n",
    "        cls.f.write(data)\n",
    "        cls.f.write('\\n')\n",
    "    \n",
    "    @classmethod\n",
    "    def save_deque_to_file(cls):\n",
    "        cls.deq\n",
    "        while cls.deq:\n",
    "            cat, text = cls.deq.pop()\n",
    "            cls.f.write(cat)\n",
    "            cls.f.write(';')\n",
    "            cls.f.write(text)\n",
    "            cls.f.write('\\n')\n",
    "        cls.f.flush()\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_dataset(cls, start_cat=0, end_cat=len(routes)):\n",
    "        for route in cls.routes[start_cat:end_cat]:\n",
    "            category = route['category']\n",
    "            keyword = route['keyword']\n",
    "            url = 'https://ria.ru/' + keyword\n",
    "            res = req.get(url)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            for ref in cls.get_refs_news(soup):\n",
    "                # cls.save_to_file(category, cls.get_text(ref))\n",
    "                cls.deq.append((category, cls.get_text(ref)))\n",
    "                # cls.save_to_df(category, cls.get_text(ref))\n",
    "            # cls.f.flush()\n",
    "            \n",
    "            url = 'https://ria.ru' + soup.find('div', attrs={'class': 'list-more color-btn-second-hover'})['data-url']\n",
    "            res = req.get(url)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            for ref in cls.get_refs_news(soup):\n",
    "                # cls.save_to_df(category, cls.get_text(ref))\n",
    "                cls.deq.append((category, cls.get_text(ref)))\n",
    "            \n",
    "\n",
    "            for i in range(4):\n",
    "                try:\n",
    "                    url = 'https://ria.ru' + soup.find('div', attrs={'class': 'list-items-loaded'})['data-next-url']\n",
    "                    res = req.get(url)\n",
    "                    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                    for ref in cls.get_refs_news(soup):\n",
    "                        # cls.save_to_df(category, cls.get_text(ref))\n",
    "                        cls.deq.append((category, cls.get_text(ref)))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            \n",
    "            # with open('out.html', 'w', encoding='utf-8') as f:\n",
    "            #     f.write(res.text)\n",
    "            cls.save_deque_to_file()\n",
    "            print('end', category)\n",
    "\n",
    "start = time.time()\n",
    "deq = RiaParser.get_dataset()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0127881-f062-4e52-a140-a99a023d09b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 8 fields in line 3, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_clf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_parsed_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 8 fields in line 3, saw 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_clf = pd.read_csv(\"out_parsed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b7128-73ec-40d3-90aa-741938338680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
